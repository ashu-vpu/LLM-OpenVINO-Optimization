# LLM-OpenVINO-Optimization
Optimizing LLM inference using OpenVINO for efficient deployment on edge devices
